{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-29T16:22:36.145954Z",
     "start_time": "2024-09-29T16:22:36.143844Z"
    }
   },
   "source": "import re",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T16:22:36.639876Z",
     "start_time": "2024-09-29T16:22:36.637016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "In the vast expanse of space, stars form from clouds of gas and dust.\n",
    "Each galaxy holds billions of stars, many of which have planets orbiting them.\n",
    "Machine learning has revolutionized industries by enabling models to learn from data.\n",
    "Python is a widely-used programming language known for its simplicity and versatility.\n",
    "The weather today is sunny with a chance of scattered thunderstorms in the evening.\n",
    "The quick brown fox jumps over the lazy dog while the cat observes from a distance.\n",
    "In 2020, remote work became the norm, leading to an increased reliance on digital tools.\n",
    "Scientists discovered new species in the depths of the ocean, challenging our understanding of marine biology.\n",
    "Modern architecture blends functionality with sleek, minimalist design principles.\n",
    "As climate change progresses, efforts to reduce carbon emissions are intensifying.\n",
    "History shows that civilizations often collapse when resources are mismanaged or depleted.\n",
    "\"\"\""
   ],
   "id": "df7ec066aa6475e2",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T16:22:37.307501Z",
     "start_time": "2024-09-29T16:22:37.299969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EOT_TOKEN = \"<|endoftext|>\"\n",
    "UNK_TOKEN = \"<|unk|>\"\n",
    "\n",
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, stoi: dict[str, int]):\n",
    "        self.stoi = stoi\n",
    "        self.itos = {i: s for s, i in stoi.items()}\n",
    "        \n",
    "    @staticmethod\n",
    "    def _text_to_tokens(text: str) -> list[str]:\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        return [t.strip() for t in tokens if t.strip()]\n",
    "        \n",
    "    @classmethod\n",
    "    def train(cls, text: str) -> 'Tokenizer':\n",
    "        print(f\"{len(text)=}\")\n",
    "        tokens = cls._text_to_tokens(text)\n",
    "        print(f\"{tokens[:5]=}\")\n",
    "        print(f\"{len(tokens)=}\")\n",
    "        vocab = sorted(set(tokens))\n",
    "        special_tokens = [EOT_TOKEN, UNK_TOKEN]\n",
    "        print(f\"{special_tokens=}\")\n",
    "        vocab.extend(special_tokens)\n",
    "        print(f\"{len(vocab)=}\")\n",
    "        stoi = {s: i for i, s in enumerate(vocab)}\n",
    "        return cls(stoi)\n",
    "        \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        tokens = self._text_to_tokens(text)\n",
    "        tokens = [t if t in self.stoi else UNK_TOKEN for t in tokens]\n",
    "        token_ids = [self.stoi[s] for s in tokens]\n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        text = ' '.join([self.itos[i] for i in token_ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ],
   "id": "ab6b906502606124",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T16:22:49.823793Z",
     "start_time": "2024-09-29T16:22:49.818978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"{text[:50]=}\")\n",
    "tokenizer = Tokenizer.train(text)\n",
    "new_text = \"lazy dog reduce massive amount of carbon emissions. <|endoftext|> childlike or childish?\"\n",
    "print(f\"{new_text=}\")\n",
    "token_ids = tokenizer.encode(new_text)\n",
    "print(f\"{token_ids=}\")\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"{decoded=}\")"
   ],
   "id": "b4cb6c37f3fc46ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text[:50]='\\nIn the vast expanse of space, stars form from clo'\n",
      "len(text)=948\n",
      "tokens[:5]=['In', 'the', 'vast', 'expanse', 'of']\n",
      "len(tokens)=162\n",
      "special_tokens=['<|endoftext|>', '<|unk|>']\n",
      "len(vocab)=122\n",
      "new_text='lazy dog reduce massive amount of carbon emissions. <|endoftext|> childlike or childish?'\n",
      "token_ids=[65, 39, 90, 121, 121, 78, 23, 42, 1, 120, 121, 81, 121, 121]\n",
      "decoded='lazy dog reduce <|unk|> <|unk|> of carbon emissions. <|endoftext|> <|unk|> or <|unk|> <|unk|>'\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cf8545b009e2ccfe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
